# -*- coding: utf-8 -*-
"""Logistic Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q5TArH3VsAAvC-HaSGnJi1WLSm000PwR

**Importing Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

"""**Importing Data**"""

from google.colab import files
uploaded = files.upload()

df=pd.read_csv("Breast Cancer.csv")

"""# **EDA**"""

df.shape

df.head()

col_names=df.columns
col_names

"""**Drop Risk_MM variable**"""

print(df.columns)
df.drop(columns=['RISK_MM'], inplace=True, errors='ignore')

df.info()

categorical=[var for var in df.columns if df[var].dtype=='O']
print('There are {} categorical variables\n'.format(len(categorical)))
print('The categorical variables are :', categorical)

df[categorical].head()

df[categorical].isnull().sum()

for var in categorical:
    print(df[var].value_counts())

for var in categorical:
  print(df[var].value_counts()/float(len(df)))

for var in categorical:
    print(var, ' contains ', len(df[var].unique()), ' labels')

df['diagnosis'].dtypes

print('diagnosis contains', len(df.diagnosis.unique()), 'labels')

df.diagnosis.unique()

df.diagnosis.value_counts()

pd.get_dummies(df.diagnosis, drop_first=True).head()

"""**Numerical Variables**"""

numerical=[var for var in df.columns if df[var].dtype!='O']
print('There are {} numerical variables\n'.format(len(numerical)))
print('The numerical variables are :', numerical)

df[numerical].head()

df[numerical].isnull().sum()

print(round(df[numerical].describe()),2)

import matplotlib.pyplot as plt

cols=['id', 'area_se', 'perimeter_se', 'radius_se', 'area_worst', 'smoothness_se','fractal_dimension_se', 'compactness_se', 'symmetry_se',
'area_mean','fractal_dimension_worst', 'symmetry_worst', 'concavity_se', 'texture_se','concave points_se', 'concavity_mean', 'radius_worst',
'compactness_mean','compactness_worst', 'symmetry_mean']
fig, axes=plt.subplots(5, 4, figsize=(20, 20))
axes=axes.flatten()
for i, col in enumerate(cols):
    df.boxplot(column=col, ax=axes[i])
    axes[i].set_title(col, fontsize=20)
    axes[i].set_ylabel('')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
num_cols = df.select_dtypes(include=['float64', 'int64']).columns
plt.figure(figsize=(20, 20))
for i, col in enumerate(num_cols):
    plt.subplot(6, 6, i + 1)
    sns.histplot(df[col], kde=True, bins=20, color='skyblue')
    plt.title(col, fontsize=14)
    plt.xlabel('')
    plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

num_cols=df.select_dtypes(include=['float64', 'int64']).columns
for col in num_cols:
    IQR=df[col].quantile(0.75)-df[col].quantile(0.25)
    Lower_fence=df[col].quantile(0.25)-(IQR*3)
    Upper_fence=df[col].quantile(0.75)+(IQR*3)
    outliers=df[(df[col] < Lower_fence) | (df[col] > Upper_fence)][col]
    print(f"{col} -> Outliers are values < {Lower_fence:.2f} or > {Upper_fence:.2f}")
    print(f"Number of outliers: {len(outliers)}\n")

X=df.drop('diagnosis', axis=1)
y=df['diagnosis']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=0)

X_train.shape, X_test.shape

X_train.dtypes

categorical=[col for col in X_train.columns if X_train[col].dtypes == 'O']
categorical

numerical=[col for col in X_train.columns if X_train[col].dtypes != 'O']
numerical

X_train[numerical].isnull().sum()

X_test[numerical].isnull().sum()

X_train[categorical].isnull().mean()

def max_value(df, variable, top):
    return np.where(df[variable] > top, top, df[variable])
for df3 in [X_train, X_test]:
    df3['radius_mean']=max_value(df3, 'radius_mean', 25.0)
    df3['texture_mean']=max_value(df3, 'texture_mean', 30.0)
    df3['perimeter_mean']=max_value(df3, 'perimeter_mean', 180.0)
    df3['area_mean']=max_value(df3, 'area_mean', 2000.0)
    df3['smoothness_mean']=max_value(df3, 'smoothness_mean', 0.15)

X_train.radius_mean.max(), X_test.radius_mean.max()

X_train.texture_mean.max(), X_test.texture_mean.max()

X_train.perimeter_mean.max(), X_test.perimeter_mean.max()

X_train.area_mean.max(), X_test.area_mean.max()

X_train.smoothness_mean.max(), X_test.smoothness_mean.max()

X_train[numerical].describe()

X_train.describe()

cols=X_train.columns

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)

X_train=pd.DataFrame(X_train, columns=[cols])
X_test=pd.DataFrame(X_test, columns=[cols])

X_train.describe()

"""# **MODEL TRAINING**"""

from sklearn.linear_model import LogisticRegression
logreg=LogisticRegression(solver='liblinear', random_state=0)
logreg.fit(X_train, y_train)

"""**Predict Result**"""

y_pred_test=logreg.predict(X_test)
y_pred_test

logreg.predict_proba(X_test)[:,0]

logreg.predict_proba(X_test)[:,1]

from sklearn.metrics import accuracy_score
print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))

y_pred_train=logreg.predict(X_train)
y_pred_train

print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

"""**Overfitting and Underfitting**"""

print('Training set score: {:.4f}'.format(logreg.score(X_train, y_train)))
print('Test set score: {:.4f}'.format(logreg.score(X_test, y_test)))

logreg100=LogisticRegression(C=100, solver='liblinear', random_state=0)
logreg100.fit(X_train, y_train)

print('Training set score: {:.4f}'.format(logreg100.score(X_train, y_train)))
print('Test set score: {:.4f}'.format(logreg100.score(X_test, y_test)))

logreg001=LogisticRegression(C=0.01, solver='liblinear', random_state=0)
logreg001.fit(X_train, y_train)

print('Training set score: {:.4f}'.format(logreg001.score(X_train, y_train)))
print('Test set score: {:.4f}'.format(logreg001.score(X_test, y_test)))

y_test.value_counts()

null_accuracy=(67/(67+47))
print('Null accuracy score: {0:0.4f}'. format(null_accuracy))

from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_test, y_pred_test)
print('Confusion matrix\n\n', cm)
print('\nTrue Positives(TP)=', cm[0,0])
print('\nTrue Negatives(TN)=', cm[1,1])
print('\nFalse Positives(FP)=', cm[0,1])
print('\nFalse Negatives(FN)=', cm[1,0])

cm_matrix=pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], index=['Predict Positive:1', 'Predict Negative:0'])
sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_test))

TP=cm[0,0]
TN=cm[1,1]
FP=cm[0,1]
FN=cm[1,0]

classification_accuracy=(TP+TN)/float(TP+TN+FP+FN)
print('Classification accuracy:{0:0.4f}'.format(classification_accuracy))

classification_error=(FP+FN)/float(TP+TN+FP+FN)
print('Classification error:{0:0.4f}'.format(classification_error))

"""**Precision**"""

precision=TP/float(TP+FP)
print('Precision:{0:0.4f}'.format(precision))

"""**Recall**"""

recall=TP/float(TP+FN)
print('Recall or Sensitivity:{0:0.4f}'.format(recall))

true_positive_rate=TP/float(TP+FN)
print('True Positive Rate:{0:0.4f}'.format(true_positive_rate))

false_positive_rate=FP/float(FP+TN)
print('False Positive Rate:{0:0.4f}'.format(false_positive_rate))

"""**Specificity**"""

specificity=TN/(TN+FP)
print('Specificity:{0:0.4f}'.format(specificity))

"""**Adjusting Threshold level**"""

y_pred_prob=logreg.predict_proba(X_test)[0:10]
y_pred_prob

y_pred_prob_df=pd.DataFrame(data=y_pred_prob, columns=['Prob of-diagnosis(M)', 'Prob of-diagonosis (B)'])
y_pred_prob_df

logreg.predict_proba(X_test)[0:10, 1]

y_pred1=logreg.predict_proba(X_test)[:, 1]

plt.rcParams['font.size']=12
plt.hist(y_pred1, bins=10)
plt.title('Histogram of predicted diagonosis')
plt.xlim(0,1)
plt.xlabel('Predicted diagonosis')
plt.ylabel('Frequency')

from sklearn.preprocessing import binarize
from sklearn.metrics import confusion_matrix, accuracy_score
y_test_numeric=np.where(y_test == 'Yes', 1, 0) if y_test.dtype == 'O' else y_test
for i in range(1,5):
    y_pred_prob=logreg.predict_proba(X_test)[:,1].reshape(-1,1)
    y_pred_bin=binarize(y_pred_prob, threshold=i/10)
    y_pred_label=np.where(y_pred_bin==1, 1, 0)
    cm=confusion_matrix(y_test_numeric, y_pred_label)
    print(f"With threshold {i/10} the Confusion Matrix is:\n{cm}\n")
    print(f"Correct predictions: {cm[0,0]+cm[1,1]}")
    print(f"Type I errors (False Positives): {cm[0,1]}")
    print(f"Type II errors (False Negatives): {cm[1,0]}")
    print(f"Accuracy: {accuracy_score(y_test_numeric, y_pred_label):.4f}")
    print(f"Sensitivity: {cm[1,1]/(cm[1,1]+cm[1,0]):.4f}")
    print(f"Specificity: {cm[0,0]/(cm[0,0]+cm[0,1]):.4f}")
    print("====================================================\n")

"""**ROC**"""

from sklearn.metrics import roc_curve
fpr, tpr, thresholds=roc_curve(y_test, y_pred1, pos_label='Yes')
plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, linewidth=2)
plt.plot([0,1], [0,1], 'k--' )
plt.rcParams['font.size']=12
plt.title('ROC curve for diagonosis')
plt.xlabel('False Positive Rate (1-Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.show()

from sklearn.metrics import roc_auc_score
ROC_AUC=roc_auc_score(y_test, y_pred1)
print('ROC AUC:{:.4f}'.format(ROC_AUC))

from sklearn.model_selection import cross_val_score
Cross_validated_ROC_AUC=cross_val_score(logreg, X_train, y_train, cv=5, scoring='roc_auc').mean()
print('Cross validated ROC AUC:{:.4f}'.format(Cross_validated_ROC_AUC))

"""**Recursive Feature Elimination with Cross Validation**"""

from sklearn.feature_selection import RFECV
rfecv=RFECV(estimator=logreg, step=1, cv=5, scoring='accuracy')
rfecv=rfecv.fit(X_train, y_train)

print("Optimal number of features:%d" % rfecv.n_features_)

X_train_rfecv=rfecv.transform(X_train)
logreg.fit(X_train_rfecv, y_train)

X_test_rfecv=rfecv.transform(X_test)
y_pred_rfecv=logreg.predict(X_test_rfecv)

print ("Classifier score: {:.4f}".format(logreg.score(X_test_rfecv,y_test)))

from sklearn.metrics import confusion_matrix
cm1=confusion_matrix(y_test, y_pred_rfecv)
print('Confusion matrix\n\n', cm1)
print('\nTrue Positives(TP1)=', cm1[0,0])
print('\nTrue Negatives(TN1)=', cm1[1,1])
print('\nFalse Positives(FP1)=', cm1[0,1])
print('\nFalse Negatives(FN1)=', cm1[1,0])

"""**k-Fold Cross Validation**"""

from sklearn.model_selection import cross_val_score
scores=cross_val_score(logreg, X_train, y_train, cv=5, scoring='accuracy')
print('Cross-validation scores:{}'.format(scores))

print('Average cross-validation score: {:.4f}'.format(scores.mean()))

"""**Hyperparameter Optimization using GridSearch CV**"""

from sklearn.model_selection import GridSearchCV
parameters=[{'penalty':['l1','l2']},
              {'C':[1, 10, 100, 1000]}]
grid_search=GridSearchCV(estimator=logreg,param_grid=parameters,scoring='accuracy',
cv=5,verbose=0)
grid_search.fit(X_train, y_train)

print('GridSearch CV best score:{:.4f}\n\n'.format(grid_search.best_score_))
print('Parameters that give the best results :','\n\n', (grid_search.best_params_))
print('\n\nEstimator that was chosen by the search :','\n\n', (grid_search.best_estimator_))

print('GridSearch CV score on test set: {0:0.4f}'.format(grid_search.score(X_test, y_test)))